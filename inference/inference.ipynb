{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_path = '/home/ubuntu/public/wdq_workspace/Qwen2/models/qwen/Qwen2-7B'\n",
    "lora_path = '/home/ubuntu/public/wdq_workspace/Qwen2/examples/sft/output_qwen/checkpoint-1080' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(mode_path, lora_path, prompt, content):\n",
    "    # 加载tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)\n",
    "    # 加载模型\n",
    "    model = AutoModelForCausalLM.from_pretrained(mode_path, device_map=\"auto\",torch_dtype=torch.float16, trust_remote_code=True).eval()\n",
    "    # 加载lora权重\n",
    "    model = PeftModel.from_pretrained(model, model_id=lora_path)\n",
    "    inputs = tokenizer.apply_chat_template([{\"role\": \"system\", \"content\": prompt},{\"role\": \"user\", \"content\": content}],\n",
    "                                        add_generation_prompt=True,\n",
    "                                        tokenize=True,\n",
    "                                        return_tensors=\"pt\",\n",
    "                                        return_dict=True\n",
    "                                        ).to('cuda')\n",
    "\n",
    "    gen_kwargs = {\"do_sample\": True, \"top_k\": 1}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "        print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '你是一个专业医生，以专业的知识回答用户'\n",
    "content ='请问您能提供一些关于腰椎间盘突出症病例的分析和治疗效果的建议吗？'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(mode_path, lora_path, prompt, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vLLM推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "离线推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7, \n",
    "    top_p=0.8, \n",
    "    repetition_penalty=1.05, # 控制重复词组的惩罚系数，值大于 1 会减少重复，值等于 1 表示不应用惩罚。\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "llm = LLM(\n",
    "    MODEL_PATH,\n",
    "    max_model_len=2048,  # 降低这个值以减少内存占用\n",
    "    gpu_memory_utilization=0.9  # 值越大 GPU 内存越高\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 部署GPTQ量化模型\n",
    "# llm = LLM(\n",
    "#     QUA_MODEL_PATH,\n",
    "#     quantization=\"gptq\",\n",
    "#     max_model_len=2048,  # 降低这个值以减少内存占用\n",
    "#     gpu_memory_utilization=0.9 \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"请问在治疗神经源性环咽肌失迟缓症的病例中，导尿管球囊扩张术的具体效果如何？\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个专业医生，以专业的知识回答用户。\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate outputs\n",
    "outputs = llm.generate([text], sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r},\\nGenerated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API接口推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./run_vllm.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"qwen-1.5-7b-lora\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"你是一个专业医生，以专业的知识回答用户。\"},\n",
    "        {\"role\": \"user\", \"content\": \"请问在治疗神经源性环咽肌失迟缓症的病例中，导尿管球囊扩张术的具体效果如何？\"},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "response = completion.choices[0].message.content\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
